{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3664c1fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:36:51.842379Z",
     "iopub.status.busy": "2024-12-09T16:36:51.842153Z",
     "iopub.status.idle": "2024-12-09T16:37:04.458668Z",
     "shell.execute_reply": "2024-12-09T16:37:04.457853Z"
    },
    "papermill": {
     "duration": 12.628263,
     "end_time": "2024-12-09T16:37:04.461062",
     "exception": false,
     "start_time": "2024-12-09T16:36:51.832799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/bitsandbytes0-42-0\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.14.1)\r\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes==0.42.0) (1.26.4)\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.42.0\r\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "# !pip install --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "# !pip install --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7cf54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:37:04.481216Z",
     "iopub.status.busy": "2024-12-09T16:37:04.480906Z",
     "iopub.status.idle": "2024-12-09T16:37:04.487662Z",
     "shell.execute_reply": "2024-12-09T16:37:04.486738Z"
    },
    "papermill": {
     "duration": 0.018646,
     "end_time": "2024-12-09T16:37:04.489263",
     "exception": false,
     "start_time": "2024-12-09T16:37:04.470617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing merge.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile merge.py\n",
    "import torch\n",
    "import gc\n",
    "lora_dir = '/kaggle/input/ds5-14b-recall-19/epoch_19_model/adapter.bin'\n",
    "d1 = torch.load(lora_dir,map_location='cpu')\n",
    "lora_dir = '/kaggle/input/ds5-14b-recall-19-f1/epoch_19_model/adapter.bin'\n",
    "d2 = torch.load(lora_dir,map_location='cpu')\n",
    "lora_dir = '/kaggle/input/ds5-14b-recall-19-f2/epoch_19_model/adapter.bin'\n",
    "d3 = torch.load(lora_dir,map_location='cpu')\n",
    "lora_dir = '/kaggle/input/ds5-14b-recall-19-f3/epoch_19_model/adapter.bin'\n",
    "d4 = torch.load(lora_dir,map_location='cpu')\n",
    "lora_dir = '/kaggle/input/ds5-14b-recall-19-f4/epoch_19_model/adapter.bin'\n",
    "d5 = torch.load(lora_dir,map_location='cpu')\n",
    "d = {}\n",
    "\n",
    "for k, v in d1.items():\n",
    "    v = d1[k] + d2[k] + d3[k]+ d4[k] + d5[k]\n",
    "    v = v / 5.\n",
    "    d[k] = v\n",
    "torch.save(d,\"./final_adapter.bin\")\n",
    "del d,d1,d2,d3,d4,d5\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5138e476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:37:04.508522Z",
     "iopub.status.busy": "2024-12-09T16:37:04.508273Z",
     "iopub.status.idle": "2024-12-09T16:37:32.975377Z",
     "shell.execute_reply": "2024-12-09T16:37:32.974447Z"
    },
    "papermill": {
     "duration": 28.478868,
     "end_time": "2024-12-09T16:37:32.977361",
     "exception": false,
     "start_time": "2024-12-09T16:37:04.498493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/merge.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  d1 = torch.load(lora_dir,map_location='cpu')\r\n",
      "/kaggle/working/merge.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  d2 = torch.load(lora_dir,map_location='cpu')\r\n",
      "/kaggle/working/merge.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  d3 = torch.load(lora_dir,map_location='cpu')\r\n",
      "/kaggle/working/merge.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  d4 = torch.load(lora_dir,map_location='cpu')\r\n",
      "/kaggle/working/merge.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  d5 = torch.load(lora_dir,map_location='cpu')\r\n"
     ]
    }
   ],
   "source": [
    "!python merge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4f7aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:37:32.998336Z",
     "iopub.status.busy": "2024-12-09T16:37:32.997708Z",
     "iopub.status.idle": "2024-12-09T16:37:33.003195Z",
     "shell.execute_reply": "2024-12-09T16:37:33.002492Z"
    },
    "papermill": {
     "duration": 0.017828,
     "end_time": "2024-12-09T16:37:33.005060",
     "exception": false,
     "start_time": "2024-12-09T16:37:32.987232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing merge2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile merge2.py\n",
    "import torch\n",
    "import gc\n",
    "lora_dir = '/kaggle/input/recall-14b-all-dis-72b/adapter.bin'\n",
    "d1 = torch.load(lora_dir,map_location='cpu')\n",
    "lora_dir = '/kaggle/input/recall-14b-all-dis-72b-math/adapter.bin'\n",
    "d2 = torch.load(lora_dir,map_location='cpu')\n",
    "# lora_dir = '/kaggle/input/ds5-14b-recall-19-f2/epoch_19_model/adapter.bin'\n",
    "# d3 = torch.load(lora_dir,map_location='cpu')\n",
    "# lora_dir = '/kaggle/input/ds5-14b-recall-19-f3/epoch_19_model/adapter.bin'\n",
    "# d4 = torch.load(lora_dir,map_location='cpu')\n",
    "# lora_dir = '/kaggle/input/ds5-14b-recall-19-f4/epoch_19_model/adapter.bin'\n",
    "# d5 = torch.load(lora_dir,map_location='cpu')\n",
    "d = {}\n",
    "\n",
    "for k, v in d1.items():\n",
    "    v = d1[k] + d2[k]\n",
    "    v = v / 2.\n",
    "    d[k] = v\n",
    "torch.save(d,\"./final_adapter_14b_all.bin\")\n",
    "# del d,d1,d2,d3,d4,d5\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a12848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:37:33.024920Z",
     "iopub.status.busy": "2024-12-09T16:37:33.024657Z",
     "iopub.status.idle": "2024-12-09T16:37:43.089325Z",
     "shell.execute_reply": "2024-12-09T16:37:43.088488Z"
    },
    "papermill": {
     "duration": 10.077049,
     "end_time": "2024-12-09T16:37:43.091409",
     "exception": false,
     "start_time": "2024-12-09T16:37:33.014360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/merge2.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  d1 = torch.load(lora_dir,map_location='cpu')\r\n",
      "/kaggle/working/merge2.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  d2 = torch.load(lora_dir,map_location='cpu')\r\n"
     ]
    }
   ],
   "source": [
    "!python merge2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2ceb9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:37:43.113147Z",
     "iopub.status.busy": "2024-12-09T16:37:43.112847Z",
     "iopub.status.idle": "2024-12-09T16:37:43.124073Z",
     "shell.execute_reply": "2024-12-09T16:37:43.123207Z"
    },
    "papermill": {
     "duration": 0.024096,
     "end_time": "2024-12-09T16:37:43.125794",
     "exception": false,
     "start_time": "2024-12-09T16:37:43.101698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing recall.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile recall.py\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def apk(actual, predicted, k=25):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    \n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    \n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        # first condition checks whether it is valid prediction\n",
    "        # second condition checks if prediction is not repeated\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=25):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    \n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 8\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    result = {pids[i]: em for i, em in enumerate(sentence_embeddings)}\n",
    "    return result\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "# model_path = \"/kaggle/input/qwen2.5_14b/transformers/default/1\"\n",
    "# model_path = \"/kaggle/input/qwen2.5_14b/transformers/default/1\"\n",
    "model_path = sys.argv[3]\n",
    "lora_path = sys.argv[1]\n",
    "save_path = sys.argv[2]\n",
    "emedding_path = sys.argv[4]\n",
    "print(lora_path)\n",
    "print(save_path)\n",
    "# lora_path=\"/kaggle/input/ds5-14b-recall-19/epoch_19_model/adapter.bin\"\n",
    "device='cuda:0'\n",
    "VALID = False\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "model = AutoModel.from_pretrained(model_path, quantization_config=bnb_config,device_map=device)\n",
    "config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "model = get_peft_model(model, config)\n",
    "d = torch.load(lora_path, map_location=model.device)\n",
    "model.load_state_dict(d, strict=False)\n",
    "model = model.eval()\n",
    "model_0 = model.to(device)\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model = AutoModel.from_pretrained(model_path, quantization_config=bnb_config,device_map=device)\n",
    "config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "model = get_peft_model(model, config)\n",
    "d = torch.load(lora_path, map_location=model.device)\n",
    "model.load_state_dict(d, strict=False)\n",
    "model = model.eval()\n",
    "model_1 = model.to(device)\n",
    "\n",
    "\n",
    "task_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'\n",
    "if VALID:\n",
    "    tra = pd.read_parquet(\"/kaggle/input/v1-parquet/v1_val.parquet\")\n",
    "    print(tra.shape)\n",
    "else:\n",
    "    tra = pd.read_csv(f\"{path_prefix}/test.csv\")\n",
    "    print(tra.shape)\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "if tra.shape[0]<10:\n",
    "    misconception_mapping = misconception_mapping.sample(n=25,random_state=2023)\n",
    "    \n",
    "    \n",
    "if VALID:\n",
    "    train_data = []\n",
    "    for _,row in tra.iterrows():\n",
    "        for c in ['A','B','C','D']:\n",
    "            if str(row[f\"Misconception{c}Id\"])!=\"nan\":\n",
    "                # print(row[f\"Misconception{c}Id\"])\n",
    "                real_answer_id = row['CorrectAnswer']\n",
    "                real_text = row[f'Answer{real_answer_id}Text']\n",
    "                query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{real_text}\\n###Misconcepte Incorrect answer###:{row[f'Answer{c}Text']}\"\n",
    "                row['query_text'] = get_detailed_instruct(task_description,query_text)\n",
    "                row['answer_id'] = row[f\"Misconception{c}Id\"]\n",
    "                train_data.append(copy.deepcopy(row))\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    train_df['order_index'] = list(range(len(train_df)))\n",
    "else:\n",
    "    train_data = []\n",
    "    for _,row in tra.iterrows():\n",
    "        for c in ['A','B','C','D']:\n",
    "            if c ==row['CorrectAnswer']:\n",
    "                continue\n",
    "            if f'Answer{c}Text' not in row:\n",
    "                continue\n",
    "            real_answer_id = row['CorrectAnswer']\n",
    "            real_text = row[f'Answer{real_answer_id}Text']\n",
    "            query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{real_text}\\n###Misconcepte Incorrect answer###:{row[f'Answer{c}Text']}\"\n",
    "            query_text2 = f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{real_text}\\n###Incorrect distractor answer###:{row[f'Answer{c}Text']}\"\n",
    "            row['query_text'] = get_detailed_instruct(task_description,query_text)\n",
    "            row['query_text2'] = query_text2\n",
    "            row['answer_name'] = c\n",
    "            train_data.append(copy.deepcopy(row))\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    train_df['order_index'] = list(range(len(train_df)))\n",
    "train_df.shape\n",
    "\n",
    "\n",
    "temp1 = train_df.iloc[0::2].copy()\n",
    "temp2 = train_df.iloc[1::2].copy()\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (temp1, temp2), (model_0, model_1), (tokenizer, tokenizer),('cuda:0','cuda:1'))\n",
    "train_embeddings={}\n",
    "for res in list(results):\n",
    "    train_embeddings.update(res)\n",
    "# train_embeddings = inference(train_df, model, tokenizer, device)\n",
    "\n",
    "\n",
    "# misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "# misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "\n",
    "\n",
    "# temp1 = misconception_mapping.iloc[0::2].copy()\n",
    "# temp2 = misconception_mapping.iloc[1::2].copy()\n",
    "# with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#     results = executor.map(inference, (temp1, temp2), (model_0, model_1), (tokenizer, tokenizer),('cuda:0','cuda:1'))\n",
    "# # doc_embeddings = results[0].undate(results[1])\n",
    "\n",
    "# doc_embeddings={}\n",
    "# for res in list(results):\n",
    "#     doc_embeddings.update(res)\n",
    "# doc_embeddings = inference(misconception_mapping, model, tokenizer, device)\n",
    "\n",
    "# sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "# index_text_embeddings_index = {index: paper_id for index, paper_id in\n",
    "#                                          enumerate(list(doc_embeddings.keys()))}\n",
    "\n",
    "\n",
    "with open(emedding_path,'rb') as f:\n",
    "    index_text_embeddings_index,sentence_embeddings = pickle.load(f)\n",
    "\n",
    "predicts_test = []\n",
    "predicts_probs = []\n",
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    query_id = row['order_index']\n",
    "    query_em = train_embeddings[query_id].reshape(1, -1)\n",
    "    \n",
    "    # 计算点积\n",
    "    cosine_similarity = np.dot(query_em, sentence_embeddings.T).flatten()\n",
    "    \n",
    "    # 对余弦相似度进行排序并获取前100个索引\n",
    "    sort_index = np.argsort(-cosine_similarity)\n",
    "    pids = [index_text_embeddings_index[index] for index in sort_index]\n",
    "    predicts_test.append(pids)\n",
    "    predicts_probs.append([cosine_similarity[index] for index in sort_index])\n",
    "    \n",
    "\n",
    "if VALID:\n",
    "    train_df['recall_ids'] = predicts_test\n",
    "    print(mapk([[data] for data in train_df['answer_id'].values],train_df['recall_ids'].values))\n",
    "else:\n",
    "    train_df['recall_ids'] = predicts_test\n",
    "    train_df['recall_probs'] = predicts_probs\n",
    "    train_df['MisconceptionId'] = [' '.join(map(str,c[:25])) for c in predicts_test]\n",
    "    sub = []\n",
    "    for _,row in train_df.iterrows():\n",
    "        sub.append(\n",
    "            {\n",
    "                \"QuestionId_Answer\":f\"{row['QuestionId']}_{row['answer_name']}\",\n",
    "                \"MisconceptionId\":row['MisconceptionId']\n",
    "            }\n",
    "        )\n",
    "    submission_df = pd.DataFrame(sub)\n",
    "    submission_df.to_csv(save_path, index=False)\n",
    "    train_df.to_parquet(f\"./{save_path}.parquet\",index=False)\n",
    "    print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cef58455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:37:43.146162Z",
     "iopub.status.busy": "2024-12-09T16:37:43.145907Z",
     "iopub.status.idle": "2024-12-09T16:43:55.350749Z",
     "shell.execute_reply": "2024-12-09T16:43:55.349556Z"
    },
    "papermill": {
     "duration": 372.217774,
     "end_time": "2024-12-09T16:43:55.353055",
     "exception": false,
     "start_time": "2024-12-09T16:37:43.135281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./final_adapter.bin\r\n",
      "submission1.csv\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [03:09<00:00, 23.69s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [02:08<00:00, 16.10s/it]\r\n",
      "(3, 11)\r\n",
      "Batches:   0%|                                            | 0/1 [00:00<?, ?it/s]\r\n",
      "Batches:   0%|                                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:10<00:00, 10.48s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:14<00:00, 14.73s/it]\r\n",
      "9it [00:00, 106.44it/s]\r\n",
      "Submission file created successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!python recall.py ./final_adapter.bin  submission1.csv /kaggle/input/qwen2.5_14b/transformers/default/1 /kaggle/input/final-eedi-index-text-embeddings-index/final_eedi_index_text_embeddings_index.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8e65fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:43:55.376936Z",
     "iopub.status.busy": "2024-12-09T16:43:55.376517Z",
     "iopub.status.idle": "2024-12-09T16:48:59.763839Z",
     "shell.execute_reply": "2024-12-09T16:48:59.762373Z"
    },
    "papermill": {
     "duration": 304.401954,
     "end_time": "2024-12-09T16:48:59.766398",
     "exception": false,
     "start_time": "2024-12-09T16:43:55.364444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/14b-recall-dis-coder-merge/14b_recall_dis_coder_merge.bin\r\n",
      "submission2.csv\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [02:07<00:00, 21.29s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [02:04<00:00, 20.83s/it]\r\n",
      "(3, 11)\r\n",
      "Batches:   0%|                                            | 0/1 [00:00<?, ?it/s]\r\n",
      "Batches:   0%|                                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:10<00:00, 10.49s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:16<00:00, 16.79s/it]\r\n",
      "9it [00:00, 113.60it/s]\r\n",
      "Submission file created successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!python recall.py /kaggle/input/14b-recall-dis-coder-merge/14b_recall_dis_coder_merge.bin  submission2.csv /kaggle/input/qwen2.5-coder/transformers/14b-instruct/1 /kaggle/input/final-eedi-index-text-embeddings-index-coder/final_eedi_index_text_embeddings_index_coder.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f132999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:48:59.795630Z",
     "iopub.status.busy": "2024-12-09T16:48:59.795236Z",
     "iopub.status.idle": "2024-12-09T16:54:06.168406Z",
     "shell.execute_reply": "2024-12-09T16:54:06.167258Z"
    },
    "papermill": {
     "duration": 306.39004,
     "end_time": "2024-12-09T16:54:06.170805",
     "exception": false,
     "start_time": "2024-12-09T16:48:59.780765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./final_adapter_14b_all.bin\r\n",
      "submission3.csv\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [02:11<00:00, 16.41s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [02:07<00:00, 15.89s/it]\r\n",
      "(3, 11)\r\n",
      "Batches:   0%|                                            | 0/1 [00:00<?, ?it/s]\r\n",
      "Batches:   0%|                                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:11<00:00, 11.23s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:17<00:00, 17.12s/it]\r\n",
      "9it [00:00, 111.35it/s]\r\n",
      "Submission file created successfully!\r\n"
     ]
    }
   ],
   "source": [
    "!python recall.py ./final_adapter_14b_all.bin  submission3.csv /kaggle/input/qwen2.5_14b/transformers/default/1 /kaggle/input/recall-14b-dis-70b-all-em/final_eedi_index_text_embeddings_index_14b_all.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ded82d64",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-09T16:54:06.201275Z",
     "iopub.status.busy": "2024-12-09T16:54:06.200203Z",
     "iopub.status.idle": "2024-12-09T16:54:07.401410Z",
     "shell.execute_reply": "2024-12-09T16:54:07.400139Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.219083,
     "end_time": "2024-12-09T16:54:07.403342",
     "exception": false,
     "start_time": "2024-12-09T16:54:06.184259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QuestionId_Answer                                    MisconceptionId\n",
      "0            1869_B  2306 1507 706 2181 1005 2532 1672 1516 2518 13...\n",
      "1            1869_C  2306 1507 1005 706 2532 2518 1963 1345 1054 21...\n",
      "2            1869_D  1672 328 1507 2532 1005 1516 706 2306 1941 218...\n",
      "3            1870_A  2142 2068 418 1755 1535 143 167 891 1904 2372 ...\n",
      "4            1870_B  143 891 2398 2078 2142 2068 1755 363 167 80 15...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "val = pd.read_parquet(\"submission1.csv.parquet\")\n",
    "val2 = pd.read_parquet(\"submission2.csv.parquet\")\n",
    "val3 = pd.read_parquet(\"submission3.csv.parquet\")\n",
    "val['recall_probs2'] = val2['recall_probs'].values\n",
    "val['recall_ids2'] = val2['recall_ids'].values\n",
    "val['recall_probs3'] = val3['recall_probs'].values\n",
    "val['recall_ids3'] = val3['recall_ids'].values\n",
    "res = []\n",
    "for _,row in val.iterrows():\n",
    "    score1 =  row['recall_probs']\n",
    "    score2 = row['recall_probs2']\n",
    "    score3 = row['recall_probs3']\n",
    "    score1 = (score1 - score1.min()) / (score1.max() - score1.min())\n",
    "    score2 = (score2 - score2.min()) / (score2.max() - score2.min())\n",
    "    score3 = (score3 - score3.min()) / (score3.max() - score3.min())\n",
    "    mp = {}\n",
    "    for i in range(len(score2)):\n",
    "        mp[row['recall_ids'][i]] = mp.get(row['recall_ids'][i],0) + score1[i]\n",
    "        mp[row['recall_ids2'][i]] = mp.get(row['recall_ids2'][i],0) + score2[i]\n",
    "        mp[row['recall_ids3'][i]] = mp.get(row['recall_ids3'][i],0) + score3[i]\n",
    "    mp_ = sorted(mp.items(),key=lambda x: -x[1])[:25]\n",
    "    res.append([_[0] for _ in mp_])\n",
    "val['recall_ids'] = res\n",
    "sub=[]\n",
    "for _,row in val.iterrows():\n",
    "        sub.append(\n",
    "                {\n",
    "                    \"QuestionId_Answer\":f\"{row['QuestionId']}_{row['answer_name']}\",\n",
    "                    \"MisconceptionId\":' '.join(map(str,row['recall_ids'][:25]))\n",
    "                }\n",
    "        )\n",
    "sub = pd.DataFrame(sub)\n",
    "print(sub.head(5))\n",
    "sub.to_csv(f\"submission.csv\", index=False)\n",
    "# val.to_parquet(\"rank.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca3e08",
   "metadata": {
    "papermill": {
     "duration": 0.013126,
     "end_time": "2024-12-09T16:54:07.897819",
     "exception": false,
     "start_time": "2024-12-09T16:54:07.884693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "79f9de2a3dc5f86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "45df6857fff81cd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a03b382019cafecf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b602383",
   "metadata": {
    "papermill": {
     "duration": 0.012988,
     "end_time": "2024-12-09T16:54:07.923622",
     "exception": false,
     "start_time": "2024-12-09T16:54:07.910634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5977063,
     "sourceId": 9760679,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6106566,
     "sourceId": 9933638,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6158724,
     "sourceId": 10005086,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6161934,
     "sourceId": 10009350,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6166287,
     "sourceId": 10015440,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6166743,
     "sourceId": 10015992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6180349,
     "sourceId": 10034184,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6191814,
     "sourceId": 10049688,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6229563,
     "sourceId": 10100281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6233182,
     "sourceId": 10105123,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6233628,
     "sourceId": 10105696,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6236616,
     "sourceId": 10109543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6237362,
     "sourceId": 10110496,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6241300,
     "sourceId": 10115812,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6245005,
     "sourceId": 10120831,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6247770,
     "sourceId": 10124684,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6248069,
     "sourceId": 10125149,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10130031,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6253894,
     "sourceId": 10133127,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6253911,
     "sourceId": 10133157,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6254446,
     "sourceId": 10134120,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6257950,
     "sourceId": 10139514,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6258110,
     "sourceId": 10139726,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6266046,
     "sourceId": 10150097,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6266159,
     "sourceId": 10150256,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6266385,
     "sourceId": 10150544,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 208327345,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 137378,
     "modelInstanceId": 114098,
     "sourceId": 134896,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 161088,
     "modelInstanceId": 138456,
     "sourceId": 162820,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 161088,
     "modelInstanceId": 138579,
     "sourceId": 162952,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141558,
     "sourceId": 166361,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1390.570497,
   "end_time": "2024-12-09T16:54:08.255983",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-09T16:30:57.685486",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
